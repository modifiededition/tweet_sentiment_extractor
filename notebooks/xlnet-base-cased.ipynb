{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-05T09:13:48.675520Z",
     "iopub.status.busy": "2021-12-05T09:13:48.674825Z",
     "iopub.status.idle": "2021-12-05T09:13:48.682415Z",
     "shell.execute_reply": "2021-12-05T09:13:48.681651Z",
     "shell.execute_reply.started": "2021-12-05T09:13:48.675478Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:14:24.948618Z",
     "iopub.status.busy": "2021-12-05T09:14:24.948296Z",
     "iopub.status.idle": "2021-12-05T09:14:25.143130Z",
     "shell.execute_reply": "2021-12-05T09:14:25.142436Z",
     "shell.execute_reply.started": "2021-12-05T09:14:24.948583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:14:56.318187Z",
     "iopub.status.busy": "2021-12-05T09:14:56.317913Z",
     "iopub.status.idle": "2021-12-05T09:14:56.324053Z",
     "shell.execute_reply": "2021-12-05T09:14:56.323310Z",
     "shell.execute_reply.started": "2021-12-05T09:14:56.318157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train data:  (27481, 4)\n",
      "shape of the test data:  (3534, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the train data: \", df_train.shape)\n",
    "print(\"shape of the test data: \", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:01.318978Z",
     "iopub.status.busy": "2021-12-05T09:15:01.318305Z",
     "iopub.status.idle": "2021-12-05T09:15:02.074537Z",
     "shell.execute_reply": "2021-12-05T09:15:02.073772Z",
     "shell.execute_reply.started": "2021-12-05T09:15:01.318933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train data:  (24732, 4)\n",
      "Shape of the validation data:  (2749, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val = train_test_split(df_train,test_size=0.1,random_state=42)\n",
    "\n",
    "print(\"Shape of the train data: \", X_train.shape)\n",
    "print(\"Shape of the validation data: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:06.001992Z",
     "iopub.status.busy": "2021-12-05T09:15:06.001254Z",
     "iopub.status.idle": "2021-12-05T09:15:06.037643Z",
     "shell.execute_reply": "2021-12-05T09:15:06.036704Z",
     "shell.execute_reply.started": "2021-12-05T09:15:06.001953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train data:  (24731, 4)\n",
      "Shape of the validation data:  (2749, 4)\n"
     ]
    }
   ],
   "source": [
    "# filling na values with \"\" \n",
    "X_train.dropna(inplace=True)\n",
    "X_val.dropna(inplace=True)\n",
    "\n",
    "print(\"Shape of the train data: \", X_train.shape)\n",
    "print(\"Shape of the validation data: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:11.521133Z",
     "iopub.status.busy": "2021-12-05T09:15:11.520847Z",
     "iopub.status.idle": "2021-12-05T09:15:22.530513Z",
     "shell.execute_reply": "2021-12-05T09:15:22.529746Z",
     "shell.execute_reply.started": "2021-12-05T09:15:11.521082Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# using Xl-net tokenizer to tokenize the text into input IDs that model can make sense of.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"saved_models/xlnet/xlnet_base_cased_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:22.532409Z",
     "iopub.status.busy": "2021-12-05T09:15:22.532168Z",
     "iopub.status.idle": "2021-12-05T09:15:22.584392Z",
     "shell.execute_reply": "2021-12-05T09:15:22.583544Z",
     "shell.execute_reply.started": "2021-12-05T09:15:22.532376Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"xlnet_base_cased_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:22.586082Z",
     "iopub.status.busy": "2021-12-05T09:15:22.585805Z",
     "iopub.status.idle": "2021-12-05T09:15:23.082815Z",
     "shell.execute_reply": "2021-12-05T09:15:23.082060Z",
     "shell.execute_reply.started": "2021-12-05T09:15:22.586045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['textID', 'text', 'selected_text', 'sentiment'],\n",
       "    num_rows: 24731\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "## converting train and validation pandas dataset into huggingFace Dataset format.\n",
    "\n",
    "X_train.reset_index(drop=True,inplace=True)\n",
    "X_val.reset_index(drop=True,inplace=True)\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "validation_data = Dataset.from_pandas(X_val)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:27.004417Z",
     "iopub.status.busy": "2021-12-05T09:15:27.003857Z",
     "iopub.status.idle": "2021-12-05T09:15:27.740761Z",
     "shell.execute_reply": "2021-12-05T09:15:27.740023Z",
     "shell.execute_reply.started": "2021-12-05T09:15:27.004379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive<sep> WTF facebook just cleared out my whole survey and i was on the last q, this night gets better and better what else is next?<sep><cls>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = train_data[0][\"text\"]\n",
    "question = train_data[0][\"sentiment\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:32.187993Z",
     "iopub.status.busy": "2021-12-05T09:15:32.187395Z",
     "iopub.status.idle": "2021-12-05T09:15:40.901880Z",
     "shell.execute_reply": "2021-12-05T09:15:40.900933Z",
     "shell.execute_reply.started": "2021-12-05T09:15:32.187952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895c755a18b54432a531d372a7277154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'offset_mapping', 'selected_text', 'sentiment', 'text', 'textID', 'token_type_ids'],\n",
       "    num_rows: 24731\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to tokenize each sample.\n",
    "def preprocess(example):\n",
    "  return tokenizer(\n",
    "    example[\"sentiment\"],\n",
    "    example[\"text\"],\n",
    "    return_offsets_mapping = True\n",
    ")\n",
    "\n",
    "check_dataset = train_data.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "check_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:40.903838Z",
     "iopub.status.busy": "2021-12-05T09:15:40.903508Z",
     "iopub.status.idle": "2021-12-05T09:15:47.037268Z",
     "shell.execute_reply": "2021-12-05T09:15:47.036324Z",
     "shell.execute_reply.started": "2021-12-05T09:15:40.903802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of a sequence after tokenization:  104\n"
     ]
    }
   ],
   "source": [
    "## finding the maximum length of a sequece after tokenization\n",
    "MAX_LENGTH = 0\n",
    "for i in check_dataset:\n",
    "  length = len(i[\"input_ids\"])\n",
    "  if length > MAX_LENGTH:\n",
    "    MAX_LENGTH = length\n",
    "    \n",
    "print(\"Max length of a sequence after tokenization: \", MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:47.039415Z",
     "iopub.status.busy": "2021-12-05T09:15:47.039138Z",
     "iopub.status.idle": "2021-12-05T09:15:47.051434Z",
     "shell.execute_reply": "2021-12-05T09:15:47.050576Z",
     "shell.execute_reply.started": "2021-12-05T09:15:47.039376Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "  inputs = tokenizer(\n",
    "    examples[\"sentiment\"],\n",
    "    examples[\"text\"],\n",
    "    max_length = MAX_LENGTH,\n",
    "    return_offsets_mapping = True,\n",
    "    padding = \"max_length\",\n",
    "    )\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "  #print(inputs[\"offset_mapping\"])\n",
    "\n",
    "  for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "\n",
    "    answer = examples[\"selected_text\"][i]\n",
    "    question = examples[\"sentiment\"][i]\n",
    "    context = examples[\"text\"][i]\n",
    "    \n",
    "    \n",
    "    # print(\"context: \", context)\n",
    "    # print(\"Answer: \", answer)\n",
    "    # finding the index of first character and the index of last character of answer in the context(tweet_text)\n",
    "    \n",
    "    start_char = 0\n",
    "    end_char  = 0\n",
    "    for idx,ch in enumerate(context):\n",
    "      count = idx\n",
    "      flag = True\n",
    "      for j in answer:\n",
    "        if context[count] == j:\n",
    "          count +=1\n",
    "        else:\n",
    "          flag = False\n",
    "          break\n",
    "      if flag:\n",
    "        start_char = idx\n",
    "        break\n",
    "\n",
    "    end_char = start_char + len(answer)\n",
    "    # print(\"Input ids: \", inputs[\"input_ids\"])\n",
    "    # print(\"*\"*200)\n",
    "    # print((tokenizer.decode(inputs[\"input_ids\"][i])))\n",
    "    # print(\"*\"*200)\n",
    "    # print(len(inputs[\"input_ids\"]))\n",
    "    # print(\"*\"*200)\n",
    "    # print((inputs.sequence_ids(i)))\n",
    "    # print(inputs[\"token_type_ids\"][i])\n",
    "    # finding the start  and end of the context\n",
    "    # sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    token_type_ids = inputs[\"token_type_ids\"][i]\n",
    "\n",
    "    idx = 0\n",
    "    try:\n",
    "      while token_type_ids[idx]!=1:\n",
    "        idx+=1\n",
    "      context_start = idx\n",
    "\n",
    "      while token_type_ids[idx] ==1:\n",
    "        idx+=1\n",
    "        if idx == len(token_type_ids):\n",
    "          break\n",
    "      context_end = idx-1\n",
    "    except:\n",
    "      print(token_type_ids)\n",
    "    # finding the start position \n",
    "    # print(\"Shart Char: \", start_char)\n",
    "    # print(\"End char: \", end_char)\n",
    "    # print(\"Context Start: \", context_start)\n",
    "    # print(\"Context end: \", context_end)\n",
    "\n",
    "    idx = context_start\n",
    "    while idx <= context_end and offset[idx][0] <= start_char:\n",
    "      idx+=1\n",
    "    start_positions.append(idx-1)\n",
    "    \n",
    "    idx = context_end -1\n",
    "    while idx >= context_start and offset[idx][1] >= end_char:\n",
    "      idx-=1\n",
    "    end_positions.append(idx+1)\n",
    "  \n",
    "  inputs[\"start_positions\"] = start_positions\n",
    "  inputs[\"end_positions\"] = end_positions\n",
    "   \n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:15:48.498138Z",
     "iopub.status.busy": "2021-12-05T09:15:48.497588Z",
     "iopub.status.idle": "2021-12-05T09:16:18.968363Z",
     "shell.execute_reply": "2021-12-05T09:16:18.967554Z",
     "shell.execute_reply.started": "2021-12-05T09:15:48.498082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12679c6023a043d7a92050d7a92b0abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'end_positions', 'input_ids', 'offset_mapping', 'selected_text', 'sentiment', 'start_positions', 'text', 'textID', 'token_type_ids'],\n",
       "    num_rows: 24731\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_data = train_data.map(preprocess_training_examples,batched=True)\n",
    "processed_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:16:18.970447Z",
     "iopub.status.busy": "2021-12-05T09:16:18.970188Z",
     "iopub.status.idle": "2021-12-05T09:16:18.981183Z",
     "shell.execute_reply": "2021-12-05T09:16:18.980373Z",
     "shell.execute_reply.started": "2021-12-05T09:16:18.970411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: not fair!!\n",
      "labels give: not fair!!\n"
     ]
    }
   ],
   "source": [
    "idx = 2000\n",
    "answer = processed_train_data[idx][\"selected_text\"]\n",
    "\n",
    "start = processed_train_data[idx][\"start_positions\"]\n",
    "end = processed_train_data[idx][\"end_positions\"]\n",
    "labeled_answer = tokenizer.decode(processed_train_data[idx][\"input_ids\"][start : end +1 ])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}\")\n",
    "print(f\"labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:16:18.982552Z",
     "iopub.status.busy": "2021-12-05T09:16:18.982305Z",
     "iopub.status.idle": "2021-12-05T09:16:22.462127Z",
     "shell.execute_reply": "2021-12-05T09:16:22.461330Z",
     "shell.execute_reply.started": "2021-12-05T09:16:18.982516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac42b2ce209429689bbf0a70706b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'end_positions', 'input_ids', 'offset_mapping', 'selected_text', 'sentiment', 'start_positions', 'text', 'textID', 'token_type_ids'],\n",
       "    num_rows: 2749\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_val_data = validation_data.map(preprocess_training_examples, batched = True)\n",
    "processed_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:16:22.464475Z",
     "iopub.status.busy": "2021-12-05T09:16:22.464209Z",
     "iopub.status.idle": "2021-12-05T09:16:42.658377Z",
     "shell.execute_reply": "2021-12-05T09:16:42.657511Z",
     "shell.execute_reply.started": "2021-12-05T09:16:22.464431Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLNetForQuestionAnsweringSimple.\n",
      "\n",
      "All the layers of TFXLNetForQuestionAnsweringSimple were initialized from the model checkpoint at saved_models/xlnet/xlnet_base_cased_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetForQuestionAnsweringSimple for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"saved_models/xlnet/xlnet_base_cased_model\",local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:16:42.660039Z",
     "iopub.status.busy": "2021-12-05T09:16:42.659786Z",
     "iopub.status.idle": "2021-12-05T09:16:42.964093Z",
     "shell.execute_reply": "2021-12-05T09:16:42.963315Z",
     "shell.execute_reply.started": "2021-12-05T09:16:42.660003Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_train_dataset = processed_train_data.to_tf_dataset(\n",
    "    columns=[\n",
    "        \"input_ids\",\n",
    "        \"start_positions\",\n",
    "        \"end_positions\",\n",
    "        \"attention_mask\",\n",
    "        \"token_type_ids\"\n",
    "    ],\n",
    "    dummy_labels=True,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:16:42.967256Z",
     "iopub.status.busy": "2021-12-05T09:16:42.965320Z",
     "iopub.status.idle": "2021-12-05T09:16:43.006019Z",
     "shell.execute_reply": "2021-12-05T09:16:43.005341Z",
     "shell.execute_reply.started": "2021-12-05T09:16:42.967221Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_eval_dataset = processed_val_data.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\",\"token_type_ids\"],\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:16:43.008178Z",
     "iopub.status.busy": "2021-12-05T09:16:43.007836Z",
     "iopub.status.idle": "2021-12-05T09:16:43.026940Z",
     "shell.execute_reply": "2021-12-05T09:16:43.026225Z",
     "shell.execute_reply.started": "2021-12-05T09:16:43.008141Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_epochs = 10\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=3e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.001,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "#tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T09:16:45.910010Z",
     "iopub.status.busy": "2021-12-05T09:16:45.909144Z",
     "iopub.status.idle": "2021-12-05T10:28:39.310473Z",
     "shell.execute_reply": "2021-12-05T10:28:39.309779Z",
     "shell.execute_reply.started": "2021-12-05T09:16:45.909960Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "earlyStop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta=0, patience=1, verbose=0,\n",
    "    mode='auto',baseline=None, restore_best_weights=True\n",
    "    )\n",
    "# We're going to do validation afterwards, so no validation mid-training\n",
    "model.fit(tf_train_dataset, epochs=num_train_epochs,callbacks = [earlyStop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:47:00.322818Z",
     "iopub.status.busy": "2021-12-05T10:47:00.322062Z",
     "iopub.status.idle": "2021-12-05T10:47:00.341617Z",
     "shell.execute_reply": "2021-12-05T10:47:00.340765Z",
     "shell.execute_reply.started": "2021-12-05T10:47:00.322770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfxl_net_for_question_answering_simple\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFXLNetMainLaye multiple                  116718336 \n",
      "_________________________________________________________________\n",
      "qa_outputs (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 116,719,874\n",
      "Trainable params: 116,719,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:47:47.781330Z",
     "iopub.status.busy": "2021-12-05T10:47:47.781032Z",
     "iopub.status.idle": "2021-12-05T10:47:48.633400Z",
     "shell.execute_reply": "2021-12-05T10:47:48.632592Z",
     "shell.execute_reply.started": "2021-12-05T10:47:47.781299Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"xlnet_base_cased_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:56:46.475025Z",
     "iopub.status.busy": "2021-12-05T10:56:46.474741Z",
     "iopub.status.idle": "2021-12-05T10:56:46.489969Z",
     "shell.execute_reply": "2021-12-05T10:56:46.489160Z",
     "shell.execute_reply.started": "2021-12-05T10:56:46.474997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['textID', 'text', 'sentiment'],\n",
       "    num_rows: 3534\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "test_data = Dataset.from_pandas(df_test)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:57:40.839654Z",
     "iopub.status.busy": "2021-12-05T10:57:40.839083Z",
     "iopub.status.idle": "2021-12-05T10:57:40.845554Z",
     "shell.execute_reply": "2021-12-05T10:57:40.844807Z",
     "shell.execute_reply.started": "2021-12-05T10:57:40.839616Z"
    }
   },
   "outputs": [],
   "source": [
    "def post_porocess_data(examples):\n",
    "  questions = examples[\"sentiment\"]\n",
    "  context = examples[\"text\"]\n",
    "  inputs = tokenizer(\n",
    "      questions,\n",
    "      context,\n",
    "      max_length = MAX_LENGTH,\n",
    "      padding=\"max_length\",\n",
    "      return_offsets_mapping = True,   \n",
    "  )\n",
    "\n",
    "  for i in range(len(inputs[\"input_ids\"])):\n",
    "    offset = inputs[\"offset_mapping\"][i]\n",
    "    token_type_ids = inputs[\"token_type_ids\"][i]\n",
    "    inputs[\"offset_mapping\"][i] = [\n",
    "                                  o if token_type_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "    ]\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:57:41.650794Z",
     "iopub.status.busy": "2021-12-05T10:57:41.650259Z",
     "iopub.status.idle": "2021-12-05T10:57:44.596703Z",
     "shell.execute_reply": "2021-12-05T10:57:44.595977Z",
     "shell.execute_reply.started": "2021-12-05T10:57:41.650759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7068b1233a5e4577943617ae3997efed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'offset_mapping', 'selected_text', 'sentiment', 'text', 'textID', 'token_type_ids'],\n",
       "    num_rows: 2749\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_val_data = validation_data.map(\n",
    "    post_porocess_data,\n",
    "    batched = True\n",
    ")\n",
    "\n",
    "processed_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:59:43.436254Z",
     "iopub.status.busy": "2021-12-05T10:59:43.435976Z",
     "iopub.status.idle": "2021-12-05T10:59:43.471179Z",
     "shell.execute_reply": "2021-12-05T10:59:43.470501Z",
     "shell.execute_reply.started": "2021-12-05T10:59:43.436225Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_eval_dataset = processed_val_data.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\",\"token_type_ids\"],\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {attention_mask: (None, None), token_type_ids: (None, None), input_ids: (None, None)}, types: {attention_mask: tf.int64, token_type_ids: tf.int64, input_ids: tf.int64}>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:57:59.397339Z",
     "iopub.status.busy": "2021-12-05T10:57:59.396946Z",
     "iopub.status.idle": "2021-12-05T10:58:01.937597Z",
     "shell.execute_reply": "2021-12-05T10:58:01.936677Z",
     "shell.execute_reply.started": "2021-12-05T10:57:59.397296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24285a9ca7044a5aa6b2e8086680dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'offset_mapping', 'sentiment', 'text', 'textID', 'token_type_ids'],\n",
       "    num_rows: 3534\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test_data = test_data.map(\n",
    "    post_porocess_data,\n",
    "    batched = True\n",
    ")\n",
    "\n",
    "processed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:59:47.201009Z",
     "iopub.status.busy": "2021-12-05T10:59:47.200457Z",
     "iopub.status.idle": "2021-12-05T10:59:47.238750Z",
     "shell.execute_reply": "2021-12-05T10:59:47.237985Z",
     "shell.execute_reply.started": "2021-12-05T10:59:47.200969Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_test_dataset = processed_test_data.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\",\"token_type_ids\"],\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T10:59:50.838943Z",
     "iopub.status.busy": "2021-12-05T10:59:50.838678Z",
     "iopub.status.idle": "2021-12-05T11:00:01.247418Z",
     "shell.execute_reply": "2021-12-05T11:00:01.246065Z",
     "shell.execute_reply.started": "2021-12-05T10:59:50.838910Z"
    }
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[416,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tfxl_net_for_question_answering_simple/transformer/layer_._3/rel_attn/einsum_3/Einsum (defined at C:\\Users\\Ashish\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_tf_xlnet.py:301) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_7954]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tfxl_net_for_question_answering_simple/transformer/layer_._3/rel_attn/einsum_3/Einsum:\n tfxl_net_for_question_answering_simple/transformer/dropout_36/Identity_1 (defined at C:\\Users\\Ashish\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_tf_xlnet.py:741)\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f83031c58f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_eval_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[416,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tfxl_net_for_question_answering_simple/transformer/layer_._3/rel_attn/einsum_3/Einsum (defined at C:\\Users\\Ashish\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_tf_xlnet.py:301) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_7954]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tfxl_net_for_question_answering_simple/transformer/layer_._3/rel_attn/einsum_3/Einsum:\n tfxl_net_for_question_answering_simple/transformer/dropout_36/Identity_1 (defined at C:\\Users\\Ashish\\anaconda3\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_tf_xlnet.py:741)\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "outputs = model.predict(tf_eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-12-05T10:58:39.980466Z",
     "iopub.status.idle": "2021-12-05T10:58:39.981086Z",
     "shell.execute_reply": "2021-12-05T10:58:39.980872Z",
     "shell.execute_reply.started": "2021-12-05T10:58:39.980848Z"
    }
   },
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "n_best = 20\n",
    "\n",
    "def predict_answers(start_logits,end_logits, inputs, examples):\n",
    "    predicted_answers = []\n",
    "    for i in range(len(examples[\"textID\"])):\n",
    "        start_logit = start_logits[i]\n",
    "        end_logit = end_logits[i]\n",
    "        context = examples[\"text\"][i]\n",
    "\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        start_indexes = np.argsort(start_logit)[-1: -n_best - 1:-1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "\n",
    "        flag = False\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # skip answer that are not in the context.\n",
    "                if offset[start_index] is None or offset[end_index] is None:\n",
    "                    continue\n",
    "                # skip answer with length that is either < 0\n",
    "                if end_index < start_index:\n",
    "                    continue\n",
    "\n",
    "                flag = True\n",
    "                answer = context[offset[start_index][0]: offset[end_index][1]]\n",
    "                predicted_answers.append(answer)\n",
    "                break\n",
    "            if flag:\n",
    "                break\n",
    "        if not flag:\n",
    "            predicted_answers.append(context)\n",
    "    return predicted_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val_answers = predict_answers(start_logits, end_logits, processed_val_data, validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
